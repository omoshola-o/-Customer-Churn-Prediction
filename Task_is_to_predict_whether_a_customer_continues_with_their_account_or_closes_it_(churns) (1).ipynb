{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6308cbd3-fe9e-45c5-a473-f5a3e26169d3",
     "showTitle": false,
     "title": ""
    },
    "id": "0kX-md8RrtIF"
   },
   "source": [
    "##Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca393f63-81f7-4a8c-8793-fbce2dde85de",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2vsSWnOxi4X",
    "outputId": "0debdf19-dcdb-4aed-d97f-e8c059e8c288"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install tensorflow\n",
    "%pip install -U tensorflow\n",
    "%pip install scikeras\n",
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bb786a-1533-4be5-ad07-ffadb0bb9a77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67662475-ec0f-4b30-825f-d4379ea97f7e",
     "showTitle": false,
     "title": ""
    },
    "id": "XfxIzEXqq6ga"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d6ce5f-5658-4cd3-a7a1-983381bd93be",
     "showTitle": false,
     "title": ""
    },
    "id": "pFqDpkfBr6Vx"
   },
   "source": [
    "##Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283ac36b-7f90-44c1-8252-67eabf661952",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read train.csv\n",
    "train_ps_df = spark.read.csv(\"dbfs:/FileStore/tables/Series/train_with_ext_indicator.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Read test.csv\n",
    "test_ps_df = spark.read.csv(\"dbfs:/FileStore/tables/Series/test_with_ext_indicator.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Read sample_submission.csv\n",
    "submission_ps_df = spark.read.csv(\"dbfs:/FileStore/tables/Series/sample_submission.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7817bf-fd8b-48ea-9d5f-83a31be524dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_ps_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3e9063-7e60-46e0-9f17-01eccf12c865",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Spark DataFrames to pandas DataFrames\n",
    "df_train = train_ps_df.toPandas()\n",
    "df_test = test_ps_df.toPandas()\n",
    "df_submission = submission_ps_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edf6781-1382-4928-8bd3-bc08bd60bf71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fbb6d74-1f72-4548-b281-39307831569e",
     "showTitle": false,
     "title": ""
    },
    "id": "seU6B8WVr5Jw"
   },
   "source": [
    "## Univarite EDA of Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "398d3436-eba3-49d4-8602-c8b2e7ce2d70",
     "showTitle": false,
     "title": ""
    },
    "id": "AodlMxOJx0NV"
   },
   "source": [
    "Univariate Exploratory Data Analysis (EDA) focuses on examining each variable in isolation to summarize and find patterns in the train dataset.\n",
    "\n",
    "**To Do:**\n",
    "> *Understanding Variable Types:* Identifying whether variables are numerical (continuous or discrete) or categorical (ordinal or nominal).\n",
    "\n",
    "> *Summary Statistics:* For numerical variables, we'll look at measures like mean, median, mode, range, variance, and standard deviation. For categorical variables, we'll identify the number of categories and count the frequency of each category.\n",
    "\n",
    "> *Visualization:* We'll create visualizations such as histograms, boxplots, or bar charts to understand the distribution of each variable.\n",
    "\n",
    "> *Identifying Anomalies:* Detecting any outliers or unusual data points.\n",
    "\n",
    "> *Missing Values* Assessing if there are any missing values in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e575c4-b01c-4a1c-9e5e-5f253a5cb869",
     "showTitle": false,
     "title": ""
    },
    "id": "fWewZ8Tusg7Q"
   },
   "source": [
    "### Dataset Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "785902a1-e258-4eb5-bf73-1c42a38ce339",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RG5y4bNasYGm",
    "outputId": "a21b3500-da87-47d9-f1cb-5e1da9cd9904"
   },
   "outputs": [],
   "source": [
    "df_head = df_train.head()\n",
    "df_col = df_train.columns\n",
    "df_head, df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eec314d0-ce2c-45af-ad8a-521818bbf2f4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkjGvd3uRuDY",
    "outputId": "964c1a8a-5d5e-4b36-bf26-8054866c5517"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of train_data:\", df_train.shape)\n",
    "print(\"Shape of test_data:\", df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38c7d91-62b4-4965-ab24-3e52f9f8fed4",
     "showTitle": false,
     "title": ""
    },
    "id": "8PrRJTIx2ERG"
   },
   "source": [
    "### Data Dictionary\n",
    ">  *id:* A numerical identifier for each record.\n",
    "\n",
    ">  *CustomerId:* A unique number assigned to each customer.\n",
    "\n",
    ">  *Surname:* The surname of the customer.\n",
    "\n",
    ">  *CreditScore:* A numerical value representing the customer's credit score.\n",
    "\n",
    ">  *Geography:* The country of the customer.\n",
    "\n",
    ">  *Gender:* The gender of the customer.\n",
    "\n",
    ">  *Age:* The age of the customer.\n",
    "\n",
    ">  *Tenure:* The number of years the customer has been with the bank.\n",
    "\n",
    ">  *Balance:* The account balance of the customer.\n",
    "\n",
    ">  *NumOfProducts:* The number of products the customer has with the bank.\n",
    "\n",
    ">  *HasCrCard:* Indicates whether the customer has a credit card (1) or not (0).\n",
    "\n",
    ">  *IsActiveMember:* Indicates whether the customer is an active member (1) or not (0).\n",
    "\n",
    ">  *EstimatedSalary:* The estimated salary of the customer.\n",
    "\n",
    ">  *Exited:* Indicates whether the customer has exited (1) or not (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b37628-c216-4d25-9af0-acd87e4f258a",
     "showTitle": false,
     "title": ""
    },
    "id": "LQmXx32B3YPy"
   },
   "source": [
    "###Variable Types and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b80b8d-00d9-4d0c-9b92-3dc5324669de",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PpItfWMWy8DY",
    "outputId": "60842a2b-616a-4bce-d5fb-d4ca9a419c5c"
   },
   "outputs": [],
   "source": [
    "# Identifying variable types and checking for missing values\n",
    "variable_types = df_train.dtypes\n",
    "missing_values = df_train.isnull().sum()\n",
    "\n",
    "# Summarize the findings\n",
    "variable_types, missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbba4668-709e-4a5a-a125-322ebd37ce2b",
     "showTitle": false,
     "title": ""
    },
    "id": "d99uYu_CAYt6"
   },
   "source": [
    "In the above output,\n",
    "> **Numerical Variables**  are: id, CustomerId, CreditScore, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited.\n",
    "\n",
    ">**Categorical Variables** are: Surname, Geography, Gender.\n",
    "\n",
    "> and No missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2852da51-ba34-4fe8-98f4-31a005125464",
     "showTitle": false,
     "title": ""
    },
    "id": "N06MxXdm7r55"
   },
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73fecf0f-f666-4a94-8aec-2e984a13cdde",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "W8OQ7SLM3o8A",
    "outputId": "5c4a7f08-4881-4b6c-9a4d-e681259995c9"
   },
   "outputs": [],
   "source": [
    "# Summary statistics for numerical variables\n",
    "numerical_summary = df_train.describe()\n",
    "# Summarize the findings\n",
    "numerical_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c546e0d4-b149-4232-b080-41ff5a961d85",
     "showTitle": false,
     "title": ""
    },
    "id": "mjBvEOOJ4qJK"
   },
   "source": [
    "Summary Statistics for Numerical Variables\n",
    "\n",
    "> **CreditScore:** Ranges from 350 to 850.\n",
    "\n",
    "> **Age:** Ranges from 18 to 92 years.\n",
    "\n",
    "> **Tenure:** Ranges from 0 to 10 years.\n",
    "\n",
    "> **Balance:** Ranges from 0 to 250,898.09.\n",
    "\n",
    "> **NumOfProducts:** Ranges from 1 to 4 products.\n",
    "\n",
    "> **HasCrCard and IsActiveMember:** Binary variables (0 or 1).\n",
    "\n",
    "> **EstimatedSalary:** Ranges from 11.58 to 199,992.48.\n",
    "\n",
    "> **Exited:** Indicates customer churn (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd203b5-a65b-4f80-86e7-df161a0e0c03",
     "showTitle": false,
     "title": ""
    },
    "id": "OqwMxaJk6opN"
   },
   "source": [
    "### Visualization for Distribution of the the target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b4b10c-018a-4870-b099-82eff4c20e3d",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "1yHKQxlmTTHl",
    "outputId": "94421774-b6fb-4037-cbdb-292d087a3cce"
   },
   "outputs": [],
   "source": [
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Count plot for 'Exited' column\n",
    "sns.countplot(x='Exited', data=df_train, hue='Exited', palette='Blues', ax=axes[1])\n",
    "axes[1].set_title('Count Plot of Exited')\n",
    "\n",
    "# Pie chart for 'Exited' column\n",
    "status_counts = df_train['Exited'].value_counts()\n",
    "axes[0].pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=90, colors=sns.color_palette('Blues'))\n",
    "axes[0].set_title('Distribution of Exited')\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8eec5e-23e9-42f2-8b6b-635f80979348",
     "showTitle": false,
     "title": ""
    },
    "id": "ARrkYgWbVnpK"
   },
   "source": [
    "> The above pie chart displays the distribution of the \"Exited\" class. The light blue segment represents 78.8% of instances labeled as '0' (did not exit), while the dark blue segment represents 21.2% labeled as '1' (exited). Most data points fall into the 'did not exit' category, with the 'exited' category being smaller.\n",
    "\n",
    "> The bar graph shows the count of instances for each class. it reinforces the observation that most data points belong to the 'did not exit' class. The class imbalance may impact model performance and will be addressed.\n",
    "\n",
    "> SMOTE can improve classifier sensitivity for the minority class by balancing the classes. It enhances generalization by helping the model learn more general features of each class instead of overfitting to the majority class. SMOTE creates synthetic examples by selecting similar examples in the feature space, drawing a line between them, and generating new examples along that line. This expands the feature space for the minority class, achieving better class distribution balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8cedc3b-2d4b-47f1-8811-6a41317aacbc",
     "showTitle": false,
     "title": ""
    },
    "id": "x8CebD-TfYvh"
   },
   "outputs": [],
   "source": [
    "# Filter out the numerical columns\n",
    "numerical_columns = ['CreditScore','Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard','IsActiveMember', 'EstimatedSalary', 'Exited', 'CUUR0000SA0R_change','DFF_change', 'HOUST_change', 'MPRIME_change', 'UNRATE_change']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db54a818-8e3e-4be9-a859-342e44a21043",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for_ratio_numerical_columns1 =  ['CreditScore','Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard','IsActiveMember', 'EstimatedSalary']\n",
    "for_ratio_numerical_columns2 =  ['CUUR0000SA0R_change','DFF_change', 'HOUST_change', 'MPRIME_change', 'UNRATE_change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34d86966-218d-4b91-ae21-53433d991d1b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "4O7jUjzzetob",
    "outputId": "b8d35a3c-c966-4d36-bd42-f04c6682ac5c"
   },
   "outputs": [],
   "source": [
    "#histograms by Ratio information\n",
    "plt.figure(figsize=(12, 25))\n",
    "\n",
    "# Calculating the ratio of exited to not-exited for each bin in the histograms\n",
    "for i, column in enumerate(for_ratio_numerical_columns1, 1):\n",
    "    plt.subplot(len(for_ratio_numerical_columns1), 1, i)\n",
    "    # Plotting the histogram for both Exited = 1 and Exited = 0\n",
    "    sns.histplot(df_train[df_train['Exited'] == 1], x=column, color='red', label='Exited', kde=True, stat='density')\n",
    "    sns.histplot(df_train[df_train['Exited'] == 0], x=column, color='blue', label='Not Exited', kde=True, stat='density')\n",
    "    plt.legend()\n",
    "    plt.title(f'Distribution of {column} with Exit Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3b4a23-447c-4389-904d-09f07e5d1eb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#histograms by Ratio information\n",
    "plt.figure(figsize=(12, 25))\n",
    "\n",
    "# Calculating the ratio of exited to not-exited for each bin in the histograms\n",
    "for i, column in enumerate(for_ratio_numerical_columns2, 1):\n",
    "    plt.subplot(len(for_ratio_numerical_columns2), 1, i)\n",
    "    # Plotting the histogram for both Exited = 1 and Exited = 0\n",
    "    sns.histplot(df_train[df_train['Exited'] == 1], x=column, color='red', label='Exited', kde=True, stat='density')\n",
    "    sns.histplot(df_train[df_train['Exited'] == 0], x=column, color='blue', label='Not Exited', kde=True, stat='density')\n",
    "    plt.legend()\n",
    "    plt.title(f'Distribution of {column} with Exit Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526ea3f9-3a22-4f68-b273-6887698a587a",
     "showTitle": false,
     "title": ""
    },
    "id": "ISImLPL4h47Y"
   },
   "source": [
    "> The distribution of credit scores and age shows that higher credit scores and younger age are associated with customers staying with the bank. This suggests that creditworthiness and age play a role in customer retention. On the other hand, older customers are more likely to exit the bank, indicating age as a significant factor in customer churn.\n",
    "\n",
    "> However, there is no clear pattern indicating that tenure significantly impacts the likelihood of exiting or staying, while customers with higher balances are slightly more likely to exit. This suggests that financial stability alone does not guarantee customer loyalty.\n",
    "\n",
    ">Additionally, customers using around two products show a peak in staying, but there is a significant spike in exits for customers using three products or more. Lastly, salary does not appear to be a significant factor in customer churn. Understanding these patterns can help the bank tailor strategies to reduce churn and retain valuable customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfbd1023-45a9-4dc8-b8b7-139bf0ed86c1",
     "showTitle": false,
     "title": ""
    },
    "id": "6q4I1zjLUsr5"
   },
   "source": [
    "### Visualization for Distribution of the Numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4373002-094e-466e-85ae-398833c8c802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Histogram of numerical variables\n",
    "plt.figure(figsize=(18, 12)) # Increase figure size to (18, 12)\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_variables = len(numerical_columns)\n",
    "num_rows = math.ceil(num_variables / 3)\n",
    "num_cols = min(num_variables, 3)\n",
    "\n",
    "for i, var in enumerate(numerical_columns, 1):\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    plt.hist(df_train[var], bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of {var}')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b521c0f1-da03-4d32-a243-0b2027b9331c",
     "showTitle": false,
     "title": ""
    },
    "id": "BbIE3M0c5h9o"
   },
   "source": [
    "The histograms above provide insights into the distribution of each numerical variable in the dataset:\n",
    "\n",
    "> **CreditScore:** Appears normally distributed with a slight left skew.\n",
    "\n",
    "> **Age:** Shows a right-skewed distribution, indicating a larger proportion of younger customers.\n",
    "\n",
    "> **Tenure:** Fairly uniform distribution, with slight decreases at the lowest and highest tenure.\n",
    "\n",
    "> **Balance:** Significant peak at zero balance, suggesting many customers have no balance, followed by a fairly normal distribution for positive balances.\n",
    "\n",
    "> **NumOfProducts:** Majority of customers have 1 or 2 products, with few having 3 or 4.\n",
    "\n",
    " > **EstimatedSalary:** Uniformly distributed across different salary ranges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "337d7dcc-8148-4b4f-a915-dd80deed2a68",
     "showTitle": false,
     "title": ""
    },
    "id": "hrr9Y9UB7CRm"
   },
   "source": [
    "### Visualization for Distribution of the Categoriacal Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093790d1-51bf-4ab8-a8a2-a8d1f9e0ae06",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "V6Tr_Mbj6OD1",
    "outputId": "a5870096-9fd4-4584-85c7-3d2a0388f782"
   },
   "outputs": [],
   "source": [
    "# pie chart for categorical variables\n",
    "categorical_vars = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, var in enumerate(categorical_vars, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    counts = df_train[var].value_counts()\n",
    "    plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Blues'))\n",
    "    plt.title(f'{var} Pie Chart')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1ab471-2bb1-4ad7-a1cc-6c8b26bcb965",
     "showTitle": false,
     "title": ""
    },
    "id": "m2m5gdWz7L2J"
   },
   "source": [
    "The pie charts above provide insights into the distribution of the categorical variables:\n",
    "\n",
    ">  France has the largest customer base, accounting for 57.1% of the total. Among the three countries (Germany, Spain, and France), France has the highest number of customers.\n",
    "\n",
    ">  The customer base is almost evenly split between female (43.6%) and male (56.4%). While there is a slight majority of male customers, the difference is not substantial. 75.4% of customers have a credit card, while 24.6% do not. Men tend to carry more overall debt, including credit card debt, but the difference is not significant.\n",
    "\n",
    ">  50.2% of customers are active members, while 49.8% are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "297095c5-0d63-40bc-a27f-02641b748e12",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "n-fq7xewa3Rd",
    "outputId": "59dbf1ee-af01-4531-b1c1-91f2d4060575"
   },
   "outputs": [],
   "source": [
    "# Bar plot for mean Balance across Geographical Locations\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Geography', y='Balance', data=df_train, hue='Geography', palette='Blues', estimator=lambda x: sum(x) / len(x))\n",
    "plt.title('Mean Balance across Geographical Locations')\n",
    "plt.xlabel('Geography')\n",
    "plt.ylabel('Mean Balance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2804567f-edfd-4b9f-93b5-cdfdb3d5bc12",
     "showTitle": false,
     "title": ""
    },
    "id": "Og4RizOcblKH"
   },
   "source": [
    "> Germany leads in average balance, indicating German customers maintain higher account balances compared to customers in other countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fbe2008-b2ea-4919-97cf-1d5ac7bde4b3",
     "showTitle": false,
     "title": ""
    },
    "id": "ZIhQf7i08CRT"
   },
   "source": [
    "## Bivariate EDA of Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2d1ac1-5e09-4eb5-a754-1a2f25a26c81",
     "showTitle": false,
     "title": ""
    },
    "id": "TJ53Yfhj9UKo"
   },
   "source": [
    "Bivariate Exploratory Data Analysis (EDA) involves examining the relationships between two variables in the dataset.\n",
    "\n",
    "**To Do:**\n",
    "> *Numerical vs. Numerical:* Scatter plots or correlation coefficients.\n",
    "\n",
    "> *Categorical vs. Numerical:* Box plots, violin plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb3fd23-f836-42aa-a75c-4929c9e956b6",
     "showTitle": false,
     "title": ""
    },
    "id": "yf1UoSJuHUVT"
   },
   "source": [
    "###Numerical vs. Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16392276-a224-46d9-a390-ea4a55856961",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "m7sFSZz_DEkm",
    "outputId": "b70d3957-b2af-42fc-f862-2c92f0065310"
   },
   "outputs": [],
   "source": [
    "# Computing the correlation matrix\n",
    "correlation_matrix = df_train[numerical_columns].corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25fa096a-75e4-457e-b8b3-7ba7dce30dec",
     "showTitle": false,
     "title": ""
    },
    "id": "pavZVfosDZFj"
   },
   "source": [
    "> **CreditScore:** No strong correlations with other variables.\n",
    "Slight negative correlation with **Exited** (-0.02), suggesting that lower credit scores may be marginally associated with higher exit rates.\n",
    "\n",
    "> **Age:**\n",
    "Moderate positive correlation with Exited (0.30), indicating older customers are more likely to leave the bank. Weak negative correlations with NumOfProducts and IsActiveMember.\n",
    "\n",
    "> **Tenure:**\n",
    "No significant correlations with other variables.\n",
    "\n",
    "> **Balance:**\n",
    "Moderate negative correlation with NumOfProducts (-0.31), suggesting customers with more products tend to have lower balances. Slight positive correlation with Exited (0.12), implying higher balances may be slightly associated with a higher likelihood of leaving the bank.\n",
    "\n",
    "> **NumOfProducts:**\n",
    "Moderate negative correlation with Balance.\n",
    "Moderate positive correlation with IsActiveMember (0.32), suggesting active members tend to use more bank products. Strong negative correlation with Exited (-0.37), indicating customers with more products are less likely to leave.\n",
    "\n",
    "> **HasCrCard:**\n",
    "Very weak correlations with all other variables, indicating having a credit card is not strongly associated with other factors in the dataset.\n",
    "\n",
    "> **IsActiveMember:**\n",
    "Moderate positive correlation with NumOfProducts.\n",
    "Moderate negative correlation with Exited (-0.16), suggesting active members are less likely to leave the bank.\n",
    "\n",
    "> **EstimatedSalary:**\n",
    "Very weak correlations with all other variables, showing no clear pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7717f4-b2a7-41de-9378-7e217de9d505",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "sTTfSOdn9TwI",
    "outputId": "6d86b8b3-6099-4c7c-db29-7f2aeb5d00c7"
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap for numerical variables\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_train[for_ratio_numerical_columns1].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap for Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a369d675-220a-4683-80ba-af50260c26c1",
     "showTitle": false,
     "title": ""
    },
    "id": "RTs58F6CEChO"
   },
   "source": [
    "The above heatmap supports the correlation coefficient and shows that:\n",
    "\n",
    "> Age, number of products, and active membership status are the most indicative factors in relation to customer exit.\n",
    "\n",
    "> Balance shows some association with customer exit, though it's not as strong as age or number of products.\n",
    "\n",
    "> Credit score, tenure, having a credit card, and estimated salary have very weak to negligible correlations with customer exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03858222-e88f-4063-8808-50041210674a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "df_train = spark.createDataFrame(df_train)\n",
    "\n",
    "# Add 'Exited' column to df_train for correlation heatmap\n",
    "df_train = for_ratio_numerical_columns2.withColumn(\"Exited\", df_train[\"Exited\"].cast(\"double\"))\n",
    "\n",
    "# Correlation heatmap for numerical variables\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_train[for_ratio_numerical_columns2].toPandas().corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap for Numerical Variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355382b9-2c0b-4898-baa5-f247519907fb",
     "showTitle": false,
     "title": ""
    },
    "id": "7KMiwDugBB4C"
   },
   "source": [
    "### Continuous Columns Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5896e81-f7c3-4389-85f7-329b59acd7f7",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "HOYLIO6kl9le",
    "outputId": "e7cc09ac-c484-4a7a-cd91-9b7bf9d790d2"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out continuous columns\n",
    "continuous_vars = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'CUUR0000SA0R_change','DFF_change', 'HOUST_change', 'MPRIME_change', 'UNRATE_change']\n",
    "\n",
    "# Define the number of rows and columns for subplots\n",
    "num_rows = len(continuous_vars)\n",
    "num_cols = 2  # Two plots for each column (box plot and KDE plot)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 5 * num_rows))\n",
    "\n",
    "# Flatten the axes array if there's only one row\n",
    "if num_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "# Color for plots\n",
    "color = 'skyblue'\n",
    "\n",
    "# Iterate over each continuous variable and create box plots and KDE plots\n",
    "for i, column in enumerate(continuous_vars):\n",
    "    # Box plot\n",
    "    sns.boxplot(x=df_train[column], ax=axes[i, 0], color=color)\n",
    "    axes[i, 0].set_title(f'Boxplot of {column}', fontsize=14)\n",
    "    axes[i, 0].set_xlabel(column, fontsize=12)\n",
    "\n",
    "    # KDE plot\n",
    "    sns.kdeplot(data=df_train[column], ax=axes[i, 1], color=color, fill=True)\n",
    "    axes[i, 1].set_title(f'KDE Plot of {column}', fontsize=14)\n",
    "    axes[i, 1].set_xlabel(column, fontsize=12)\n",
    "    axes[i, 1].legend([column], loc='upper right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b4482b-9cf2-4c1f-b58b-173de8a86e20",
     "showTitle": false,
     "title": ""
    },
    "id": "bF2M85aJsuEw"
   },
   "source": [
    "\n",
    "> The above analysis explores credit scores, age demographics, account balances, and estimated salaries. The credit score plot indicates most people fall between 600-700, while the age plot shows a majority in late 20s to early 40s. Account balance data reveals many with zero balance and a peak around $100,000. Estimated salary plots suggest a uniform distribution with no concentration at any level. These insights could influence decisions to exit.\n",
    "\n",
    "> The Kernel Density Estimation (KDE) plots show distributions resembling normal curves, showing valuable insights for interpreting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b8ab71-430c-4a6d-a033-bc5f447ed737",
     "showTitle": false,
     "title": ""
    },
    "id": "I6VPrd2EuAS2"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef9e203-3d36-495b-96ed-002852a2e653",
     "showTitle": false,
     "title": ""
    },
    "id": "p1Wioh0DFsv_"
   },
   "source": [
    "Data Preprocessing - This is preparing the data for modelling.\n",
    "\n",
    "**To Do:**\n",
    "> *Dropong Irrelevant Features:*\n",
    "\n",
    "> *Checking for Outliers and Handling them*\n",
    "\n",
    "> *Processing each columns according to their datatypes*\n",
    "\n",
    "> *Data Transformation, Encoding and Scalling*\n",
    "\n",
    "> *Spliting the data for trainning and testing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37314d6-bc4d-4682-a5e2-7c4dbb03b61e",
     "showTitle": false,
     "title": ""
    },
    "id": "krncwYVp90LE"
   },
   "source": [
    "### Dropping Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053737e6-6626-48b7-8a60-d6c283c84874",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "rL9CBG4y9rxY",
    "outputId": "46fb6d8f-0ac1-48b3-b960-e931e80798e2"
   },
   "outputs": [],
   "source": [
    "# Dropping irrelevant features\n",
    "df_train = df_train.drop(['id', 'CustomerId', 'Surname'], axis=1)\n",
    "df_test = df_test.drop(['id', 'CustomerId', 'Surname'], axis=1)\n",
    "\n",
    "# Display the first few rows of the processed dataset\n",
    "df_train.head(), df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b51a89c8-0704-4920-8fb5-21dc1cf43a33",
     "showTitle": false,
     "title": ""
    },
    "id": "N_IY77io-EFs"
   },
   "source": [
    "> To simplify the model and potentially improve its performance, irrelevant features have been removed, Why? the ID simple row identifier,\n",
    "CustomerId is unique to each customer but doesn't hold predictive power,\n",
    "and also the customer's surname is unlikely to influence their decision to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0314020-7a56-4465-a4b9-f72967ddc7a5",
     "showTitle": false,
     "title": ""
    },
    "id": "1EoX3PXX5Z-n"
   },
   "source": [
    "### Checking for Percentage of Outliers Continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fdde8cd-2a76-403c-a62c-438710157868",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "xbPoUpDj1E7h",
    "outputId": "af17b6a9-9302-4aa6-e0d2-72d43bf58c56"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define continuous variables\n",
    "continuous_vars = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary','CUUR0000SA0R_change','DFF_change', 'HOUST_change', 'MPRIME_change', 'UNRATE_change']\n",
    "\n",
    "# Improved function to calculate percentage of outliers using IQR method\n",
    "def percentage_outliers_improved(df, columns):\n",
    "    outliers_percentage = {}\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        total_count = df[column].count()\n",
    "        outliers_count = df[column][(df[column] < lower_bound) | (df[column] > upper_bound)].count()\n",
    "        outliers_percentage[column] = (outliers_count / total_count) * 100\n",
    "\n",
    "    return outliers_percentage\n",
    "\n",
    "# Calculate the percentage of outliers for each continuous variable\n",
    "outliers_percentage_improved = percentage_outliers_improved(df_train, continuous_vars)\n",
    "outliers_percentage_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515eb761-a155-4674-9eef-1d1dfea7c397",
     "showTitle": false,
     "title": ""
    },
    "id": "74HBwXb35DTx"
   },
   "source": [
    "The highest percentage of outliers is found in 'Age', but even this is relatively low (under 4%). The low percentages of outliers in these variables suggest that the data is fairly consistent and doesn't contain many extreme values that could skew the analysis.\n",
    "\n",
    "Considering these percentages, it seems reasonable to proceed without removing these outliers, as they represent a small portion of the dataset and could contain valuable information for predicting churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1739485-9488-4356-8eb6-3a61da22c526",
     "showTitle": false,
     "title": ""
    },
    "id": "Psz4frz9_xkZ"
   },
   "source": [
    "### Process Data according to thier datatype for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f197d75-1eb7-45fb-baac-5404504d9099",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "6m3AYOn76euU",
    "outputId": "776c3149-70c1-4190-e9fc-90c049dee36b"
   },
   "outputs": [],
   "source": [
    "# Separate numerical and categorical columns for data_processed\n",
    "numerical_cols_processed = df_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols_processed = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove 'Exited' column from numerical_cols if present\n",
    "if 'Exited' in numerical_cols_processed:\n",
    "    numerical_cols_processed.remove('Exited')\n",
    "\n",
    "numerical_cols_processed, categorical_cols_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ad6901-55f4-433c-af10-8595851b1a0a",
     "showTitle": false,
     "title": ""
    },
    "id": "spYXXMUL_7gO"
   },
   "source": [
    "### Transformation, Scalling and Encoding for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beef2b7d-4139-4971-8503-2d72b725b990",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "JvPYMQuBA0sZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OrdinalEncoder\n",
    "import numpy as np\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Log transformation function\n",
    "log_transform_func = FunctionTransformer(np.log1p)\n",
    "\n",
    "# Define transformations for numerical and categorical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('ordinal_encoder', OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# Define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols_processed),\n",
    "        ('cat', categorical_transformer, categorical_cols_processed),\n",
    "        ('log', log_transform_func, ['Balance', 'EstimatedSalary'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define final pipeline\n",
    "final_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8cea81-8f17-4558-9ffb-1e491babe1ab",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "ZV4JE6e50RcI",
    "outputId": "4755afcc-2852-425a-ddb4-25f951c8596e"
   },
   "outputs": [],
   "source": [
    "final_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c984e5f3-dfc4-4034-b731-761b173041fd",
     "showTitle": false,
     "title": ""
    },
    "id": "ZHSuqG5XC5Ug"
   },
   "source": [
    "### Spliting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348a01ef-9da6-4fa9-98fb-e59518e4390f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "M1YZiPThDpSU"
   },
   "outputs": [],
   "source": [
    "# Split train data into features and target variable\n",
    "X_train = df_train.drop('Exited', axis=1)\n",
    "y_train = df_train['Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08fd52c9-e7af-4e3f-b785-c703040d2805",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "JlYyG-BbDtmJ"
   },
   "outputs": [],
   "source": [
    "# Split test data into features\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f58a60f-32cb-4a44-b70a-264f9ae49fd5",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "mH3bCDiADzDt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15fbfb4-9973-41c2-9046-adab0b489b61",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "TiRYkvyNE8KL",
    "outputId": "69eef138-a837-4678-c983-cfaef3b30dfc"
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline on the training data\n",
    "final_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ba6df6-9ddb-4847-a5e3-85f06e890b5f",
     "showTitle": false,
     "title": ""
    },
    "id": "Kvjc7a_DHvyw"
   },
   "source": [
    "### Apply Transformations on the Splited Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "175f439d-e4f5-4ce9-a0b4-f1bcd4884e31",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "VFkXEO0-h7df"
   },
   "outputs": [],
   "source": [
    "# Apply the pipeline transformations on training and validation data\n",
    "X_train_transformed = final_pipeline.transform(X_train)\n",
    "X_val_transformed = final_pipeline.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46fb2a7-9fc9-4bcb-9bc3-496f5f908c10",
     "showTitle": false,
     "title": ""
    },
    "id": "qf-QemPDGkgZ"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619a2103-f265-4e4e-b88f-231acb6c6c41",
     "showTitle": false,
     "title": ""
    },
    "id": "iCNG_6saGtu-"
   },
   "source": [
    "Modelling - This is building machine learning models for the project.\n",
    "\n",
    "**To Do:**\n",
    "> *Build a Logistic Regression Model with Hyperparameterization and Cross_Validation*\n",
    "\n",
    "> *Build a Random Forest Model with Cross Validation*\n",
    "\n",
    "> *Build a Neural Netwoorks with Cross Validation*\n",
    "\n",
    "> *Train the models, get thier Classification reports and Test them on Test Data*\n",
    "\n",
    "\n",
    "> *Plot comparison chart for the models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad09b7b5-fbb2-4a56-85b2-206f708c4804",
     "showTitle": false,
     "title": ""
    },
    "id": "VBnNnLsMKinU"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588bf775-7ee8-4242-aca9-ac6e2a149ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# Create an instance of LogisticRegression\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=logistic_model,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,\n",
    "                           scoring='roc_auc',\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Applying SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
    "\n",
    "# Fit the GridSearchCV object on the resampled training data\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Access the best hyperparameters and the best estimator\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the validation data\n",
    "lr_y_val_pred = best_model.predict(X_val_transformed)\n",
    "lr_y_val_pred_proba = best_model.predict_proba(X_val_transformed)[:, 1]\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val, lr_y_val_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "auc_score = roc_auc_score(y_val, lr_y_val_pred_proba)\n",
    "print(f\"AUC-ROC score: {auc_score}\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, lr_y_val_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, lr_y_val_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb21ce7-ba9f-4db0-99cb-dd4ed3387fd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The logistic regression model was tuned using GridSearchCV to find the optimal hyperparameters. The best parameters were L1 regularization (penalty='l1'), an inverse regularization strength of 0.1 (C=0.1), and the 'liblinear' solver. On the validation set, this model achieved an accuracy of 0.73 and an AUC-ROC score of 0.80. While the model demonstrated high precision (0.91) for the majority class (not exited), its precision for the minority class (exited) was lower at 0.42, indicating a tendency to misclassify exited customers as not exited (false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb59717-8d59-4740-85d4-19d9c4793ab8",
     "showTitle": false,
     "title": ""
    },
    "id": "YB-ydPSLIGV8"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b278967c-88b8-48d4-8a3d-479fe945b3dc",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "background_save": true
    },
    "id": "k3FDFxv1vWI4",
    "outputId": "30935d59-58af-48f9-de81-cf773993cdc6"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Create an instance of the Random Forest Classifier\n",
    "rf_classifier = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42))\n",
    "])\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(rf_classifier, X_train_transformed, y_train, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Calculate and print the mean cross-validation score\n",
    "mean_cv_score = cv_scores.mean()\n",
    "print(\"Mean cross-validation score:\", mean_cv_score)\n",
    "\n",
    "# Train the model on the entire training data\n",
    "rf_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "rf_y_val_pred = rf_classifier.predict(X_val_transformed)\n",
    "rf_y_val_pred_proba = rf_classifier.predict_proba(X_val_transformed)[:, 1]\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val, rf_y_val_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "auc_score = roc_auc_score(y_val, rf_y_val_pred_proba)\n",
    "print(f\"AUC-ROC score: {auc_score}\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, rf_y_val_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, rf_y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83b6869-dacd-46f7-9318-e5d47d6bff52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The random forest model, trained using a pipeline with SMOTE oversampling and 100 estimators, exhibited the highest performance among the three models. Cross-validation scores indicated a mean accuracy of 0.78, suggesting good generalization capabilities. On the validation set, the model achieved an accuracy of 0.78 and an AUC-ROC score of 0.82, outperforming the logistic regression and neural network models. However, similar to the logistic regression model, the random forest classifier demonstrated a higher precision (0.90) for the majority class and a lower precision (0.48) for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e9d1c9b-fa5b-405b-9e46-80be467b0b5f",
     "showTitle": false,
     "title": ""
    },
    "id": "WXLfFJ07ILvr"
   },
   "source": [
    "### Nueral Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5ec336-f7de-4a57-b7f5-63a467d1e769",
     "showTitle": false,
     "title": ""
    },
    "id": "Ji_zl7bP51nh"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Create an instance of the MLPClassifier\n",
    "# nn_model = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
    "nn_model= ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('nn', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42))\n",
    "])\n",
    "\n",
    "# Perform cross-validation on the transformed data\n",
    "scores = cross_val_score(nn_model, X_train_transformed, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean accuracy:\", scores.mean())\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "nn_model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Evaluate the best model on the validation data\n",
    "nn_y_val_pred = best_model.predict(X_val_transformed)\n",
    "nn_y_val_pred_proba = best_model.predict_proba(X_val_transformed)[:, 1]\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val, nn_y_val_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "auc_score = roc_auc_score(y_val, nn_y_val_pred_proba)\n",
    "print(f\"AUC-ROC score: {auc_score}\")\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, nn_y_val_pred))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, nn_y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f805c6c-34e5-4f62-b47f-9ade78d96ae6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The neural network model, a multi-layer perceptron with two hidden layers (100 and 50 nodes) and SMOTE oversampling, exhibited performance comparable to the logistic regression model. Cross-validation scores showed a mean accuracy of 0.76, and on the validation set, the model achieved an accuracy of 0.73 and an AUC-ROC score of 0.80, identical to the logistic regression model's performance. The classification report and confusion matrix for the neural network model were also identical to the logistic regression model, suggesting similar prediction characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6934eac-1876-4d45-bd1c-0a6d5e9ed373",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Graphical Represenstaion of Performance of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40602cb7-58ef-47c7-bd4d-eeafe1619261",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the metrics for comparison\n",
    "models = ['Logistic Regression', 'Random Forest', 'Neural Network']\n",
    "accuracy_scores = [0.8177, 0.8378, 0.8177] \n",
    "auc_roc_scores = [0.7962, 0.8205, 0.7962]   \n",
    "precision_scores = [0.65, 0.71, 0.65] \n",
    "recall_scores = [0.29, 0.39, 0.29]\n",
    "\n",
    "# Plot comparison chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.plot(models, accuracy_scores, marker='o', label='Accuracy')\n",
    "\n",
    "# AUC-ROC comparison\n",
    "plt.plot(models, auc_roc_scores, marker='o', label='AUC-ROC Score')\n",
    "\n",
    "# Precision comparison\n",
    "plt.plot(models, precision_scores, marker='o', label='Precision (Class 1)')\n",
    "\n",
    "# Recall comparison\n",
    "plt.plot(models, recall_scores, marker='o', label='Recall (Class 1)')\n",
    "\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Task_is_to_predict_whether_a_customer_continues_with_their_account_or_closes_it_(churns) (1)",
   "widgets": {}
  },
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
